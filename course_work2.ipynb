{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import Modules/Download Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\小 周\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\小 周\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\小 周\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\小 周\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scipy) (1.26.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\小 周\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n",
      "C:\\Users\\小 周\\AppData\\Local\\Temp\\ipykernel_8208\\19637222.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\小\n",
      "[nltk_data]     周\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\小\n",
      "[nltk_data]     周\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\小\n",
      "[nltk_data]     周\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "#import\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Download Models\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read a txt file to form a text dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Five types of press releases\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "data_set = []\n",
    "#Loop through the documents and add them to the array\n",
    "for category in categories:\n",
    "\n",
    "    for file_path in glob.glob(f\"./data_set/bbc/{category}/*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "            content = file.read()\n",
    "            data_set.append((content, category))\n",
    "df_data_set = pd.DataFrame(data_set, columns=[\"content\", \"category\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  category\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n"
     ]
    }
   ],
   "source": [
    "print(df_data_set.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cleansing of text data in datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Text Cleaning Functions\n",
    "def preprocess_text(text):\n",
    "    #Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    #separate word\n",
    "    tokens = word_tokenize(text)\n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #morphological restoration\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    #Returns after joining words together\n",
    "    return \" \".join(lemmatizer_tokens)\n",
    "\n",
    "\n",
    "df_data_set[\"processed_content\"] = df_data_set[\"content\"].apply(preprocess_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  category  \\\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business   \n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business   \n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business   \n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business   \n",
      "\n",
      "                                   processed_content  \n",
      "0  ad sale boost time warner profit quarterly pro...  \n",
      "1  dollar gain greenspan speech dollar hit highes...  \n",
      "2  yukos unit buyer face loan claim owner embattl...  \n",
      "3  high fuel price hit ba profit british airway b...  \n",
      "4  pernod takeover talk lift domecq share uk drin...  \n"
     ]
    }
   ],
   "source": [
    "#View the cleaned text dataset\n",
    "print(df_data_set.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "View the distribution of types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "category\nsport            511\nbusiness         510\npolitics         417\ntech             401\nentertainment    386\nName: count, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the number of press releases in each category to determine the method of dividing the data set\n",
    "df_data_set[\"category\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divide training set, development set, test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 1335 \n",
      "development dataset: 445 \n",
      "test dataset 445\n"
     ]
    }
   ],
   "source": [
    "#Divide the data set\n",
    "#training set 60%\n",
    "#Development set 20%\n",
    "#Test set 20%\n",
    "#Set a random seed to ensure that the dataset is the same each time it is divided up\n",
    "train_dev_dataset, test_dataset = train_test_split(df_data_set, test_size=0.2, random_state=10)\n",
    "train_dataset, dev_dataset = train_test_split(train_dev_dataset, test_size=0.25, random_state=10)\n",
    "#Output the length of the three sets\n",
    "print(\"train dataset:\", len(train_dataset), \"\\ndevelopment dataset:\", len(dev_dataset), \"\\ntest dataset\",\n",
    "      len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extraction characteristics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#word frequency\n",
    "count_vectorizer = CountVectorizer()\n",
    "#Learning and Transition\n",
    "train_count_vectorizer = count_vectorizer.fit_transform(train_dataset[\"processed_content\"])\n",
    "dev_count_vectorizer = count_vectorizer.transform(dev_dataset[\"processed_content\"])\n",
    "test_count_vectorizer = count_vectorizer.transform(test_dataset[\"processed_content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#binary group frequency\n",
    "N_gram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "#Learning and Transition\n",
    "train_N_gram_vectorizer = N_gram_vectorizer.fit_transform(train_dataset[\"processed_content\"])\n",
    "dev_N_gram_vectorizer = N_gram_vectorizer.transform(dev_dataset[\"processed_content\"])\n",
    "test_N_gram_vectorizer = N_gram_vectorizer.transform(test_dataset[\"processed_content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#TF-IDF features\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "#Learning and Transition\n",
    "train_tf_idf_vectorizer = tf_idf_vectorizer.fit_transform(train_dataset[\"processed_content\"])\n",
    "dev_tf_idf_vectorizer = tf_idf_vectorizer.transform(dev_dataset[\"processed_content\"])\n",
    "test_tf_idf_vectorizer = tf_idf_vectorizer.transform(test_dataset[\"processed_content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Categorised Tags\n",
    "train_category = train_dataset[\"category\"]\n",
    "dev_category = dev_dataset[\"category\"]\n",
    "test_category = test_dataset[\"category\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-count: (1335, 22370)\n",
      "N-gram: (1335, 211851)\n",
      "tf-idf: (1335, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Print the dimensions of the data\n",
    "print(\"word-count:\",train_count_vectorizer.shape)\n",
    "print(\"N-gram:\",train_N_gram_vectorizer.shape)\n",
    "print(\"tf-idf:\",train_tf_idf_vectorizer.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform feature combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#train\n",
    "train_combined_cnt = hstack([train_count_vectorizer, train_N_gram_vectorizer, train_tf_idf_vectorizer])\n",
    "train_combined_cn = hstack([train_count_vectorizer, train_N_gram_vectorizer])\n",
    "train_combined_ct = hstack([train_count_vectorizer, train_tf_idf_vectorizer])\n",
    "train_combined_nt = hstack([train_N_gram_vectorizer, train_tf_idf_vectorizer])\n",
    "#develop\n",
    "dev_combined_cnt = hstack([dev_count_vectorizer, dev_N_gram_vectorizer, dev_tf_idf_vectorizer])\n",
    "dev_combined_cn = hstack([dev_count_vectorizer, dev_N_gram_vectorizer])\n",
    "dev_combined_ct = hstack([dev_count_vectorizer, dev_tf_idf_vectorizer])\n",
    "dev_combined_nt = hstack([dev_N_gram_vectorizer, dev_tf_idf_vectorizer])\n",
    "#test\n",
    "test_combined_cnt = hstack([test_count_vectorizer, test_N_gram_vectorizer, test_tf_idf_vectorizer])\n",
    "test_combined_cn = hstack([test_count_vectorizer, test_N_gram_vectorizer])\n",
    "test_combined_ct = hstack([test_count_vectorizer, test_tf_idf_vectorizer])\n",
    "test_combined_nt = hstack([test_N_gram_vectorizer, test_tf_idf_vectorizer])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy, macro-averaged precision, macro-recall, macro-averaged f1-score function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Function feedback (label, predicted label, learning model used)\n",
    "def feedback(category, preictions, x):\n",
    "    #Accuracy\n",
    "    accuracy = accuracy_score(category, preictions)\n",
    "    #macro-averaged precision\n",
    "    macro_precision = precision_score(category, preictions, average=\"macro\")\n",
    "    #macro-recall\n",
    "    macro_recall = recall_score(category, preictions, average=\"macro\")\n",
    "    #macro-averaged f1-score\n",
    "    macro_f1 = f1_score(category, preictions, average=\"macro\")\n",
    "    #output test results\n",
    "    print(x)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"macro-averaged precision:{macro_precision * 100:.2f}%\")\n",
    "    print(f\"macro-recall:{macro_recall * 100:.2f}%\")\n",
    "    print(f\"macro-averaged f1-score:{macro_f1 * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model training function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "#MultinomialNB model\n",
    "#nb_classifier_test(Training set features, training set labels, features to be predicted, labels to be tested)\n",
    "def nb_classifier_test(train_combined, train_category, dev_combined, dev_category):\n",
    "    #Learning using features and labels\n",
    "    nb_classifier.fit(train_combined, train_category)\n",
    "    #Prediction using learnt models\n",
    "    nb_preictions = nb_classifier.predict(dev_combined)\n",
    "    feedback(dev_category, nb_preictions, \"NB:\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(random_state=11)\n",
    "\n",
    "\n",
    "#Decision Trees model\n",
    "#decision_tree_classifier_test(Training set features, training set labels, features to be predicted, labels to be tested)\n",
    "def decision_tree_classifier_test(train_combined, train_category, dev_combined, dev_category):\n",
    "    #Learning using features and labels\n",
    "    decision_tree_classifier.fit(train_combined, train_category)\n",
    "    #Prediction using learnt models\n",
    "    decision_tree_preictions = decision_tree_classifier.predict(dev_combined)\n",
    "    feedback(dev_category, decision_tree_preictions, \"Decision-Tree:\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=11)\n",
    "\n",
    "\n",
    "#Random Forest model\n",
    "#random_forest_classifier_test(Training set features, training set labels, features to be predicted, labels to be tested)\n",
    "def random_forest_classifier_test(train_combined, train_category, dev_combined, dev_category):\n",
    "    #Learning using features and labels\n",
    "    random_forest_classifier.fit(train_combined, train_category)\n",
    "    #Prediction using learnt models\n",
    "    random_forest_preictions = random_forest_classifier.predict(dev_combined)\n",
    "    feedback(dev_category, random_forest_preictions, \"Random-Forest:\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing with models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:\n",
      "Accuracy: 99.63%\n",
      "macro-averaged precision:99.61%\n",
      "macro-recall:99.62%\n",
      "macro-averaged f1-score:99.61%\n",
      "Using time:0.02 s\n",
      "\n",
      "Decision-Tree:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:0.27 s\n",
      "\n",
      "Random-Forest:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:1.05 s\n"
     ]
    }
   ],
   "source": [
    "#Detecting Models Using train Sets\n",
    "#MultinomialNB\n",
    "start_time = time.time()\n",
    "nb_classifier_test(train_count_vectorizer, train_category, train_count_vectorizer, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#Decision Trees\n",
    "start_time = time.time()\n",
    "decision_tree_classifier_test(train_count_vectorizer, train_category, train_count_vectorizer, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#Random Forest\n",
    "start_time = time.time()\n",
    "random_forest_classifier_test(train_count_vectorizer, train_category, train_count_vectorizer, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature filtering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_gram_vectorizer:\n",
      "NB:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:0.05 s\n",
      "\n",
      "tf_idf_vectorizer:\n",
      "NB:\n",
      "Accuracy: 99.03%\n",
      "macro-averaged precision:99.03%\n",
      "macro-recall:98.96%\n",
      "macro-averaged f1-score:98.99%\n",
      "Using time:0.02 s\n",
      "\n",
      "combined_cnt:\n",
      "NB:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:0.05 s\n",
      "\n",
      "combined_cn:\n",
      "NB:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:0.05 s\n",
      "\n",
      "combined_ct:\n",
      "NB:\n",
      "Accuracy: 99.70%\n",
      "macro-averaged precision:99.67%\n",
      "macro-recall:99.70%\n",
      "macro-averaged f1-score:99.69%\n",
      "Using time:0.03 s\n",
      "\n",
      "combined_nt:\n",
      "NB:\n",
      "Accuracy: 100.00%\n",
      "macro-averaged precision:100.00%\n",
      "macro-recall:100.00%\n",
      "macro-averaged f1-score:100.00%\n",
      "Using time:0.05 s\n"
     ]
    }
   ],
   "source": [
    "#N_gram_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"N_gram_vectorizer:\")\n",
    "nb_classifier_test(train_N_gram_vectorizer, train_category, train_N_gram_vectorizer, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#tf_idf_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"tf_idf_vectorizer:\")\n",
    "nb_classifier_test(train_tf_idf_vectorizer, train_category, train_tf_idf_vectorizer, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#count_vectorizer + N_gram_vectorizer + tf_idf_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"combined_cnt:\")\n",
    "nb_classifier_test(train_combined_cnt, train_category, train_combined_cnt, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#count_vectorizer + N_gram_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"combined_cn:\")\n",
    "nb_classifier_test(train_combined_cn, train_category, train_combined_cn, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#count_vectorizer + tf_idf_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"combined_ct:\")\n",
    "nb_classifier_test(train_combined_ct, train_category, train_combined_ct, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")\n",
    "print()\n",
    "#N_gram_vectorizer + tf_idf_vectorizer\n",
    "start_time = time.time()\n",
    "print(\"combined_nt:\")\n",
    "nb_classifier_test(train_combined_nt, train_category, train_combined_nt, train_category)\n",
    "end_time = time.time()\n",
    "print(f\"Using time:{end_time - start_time:.2f} s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing models for overfitting using cross-validation and development sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_vectorizer:\n",
      "Average accuracy:97.15 %\n",
      "Standard deviation of accuracy: 1.00 %\n",
      "\n",
      "tf_idf_vectorizer:\n",
      "Average accuracy:96.55 %\n",
      "Standard deviation of accuracy: 1.68 %\n",
      "\n",
      "count_vectorizer + tf_idf_vectorizer:\n",
      "Average accuracy:97.23 %\n",
      "Standard deviation of accuracy: 1.46 %\n"
     ]
    }
   ],
   "source": [
    "#train set cross-validation\n",
    "#count_vectorizer\n",
    "scores = cross_val_score(nb_classifier, train_count_vectorizer, train_category, cv=10, scoring=\"accuracy\")\n",
    "print(\"count_vectorizer:\")\n",
    "print(\"Average accuracy:%0.2f\" % (scores.mean() * 100), \"%\")\n",
    "print(\"Standard deviation of accuracy: %0.2f\" % (scores.std() * 100), \"%\")\n",
    "print()\n",
    "#tf_idf_vectorizer\n",
    "scores = cross_val_score(nb_classifier, train_tf_idf_vectorizer, train_category, cv=10, scoring=\"accuracy\")\n",
    "print(\"tf_idf_vectorizer:\")\n",
    "print(\"Average accuracy:%0.2f\" % (scores.mean() * 100), \"%\")\n",
    "print(\"Standard deviation of accuracy: %0.2f\" % (scores.std() * 100), \"%\")\n",
    "print()\n",
    "#count_vectorizer + tf_idf_vectorizer\n",
    "scores = cross_val_score(nb_classifier, train_combined_ct, train_category, cv=10, scoring=\"accuracy\")\n",
    "print(\"count_vectorizer + tf_idf_vectorizer:\")\n",
    "print(\"Average accuracy:%0.2f\" % (scores.mean() * 100), \"%\")\n",
    "print(\"Standard deviation of accuracy: %0.2f\" % (scores.std() * 100), \"%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:\n",
      "Accuracy: 97.08%\n",
      "macro-averaged precision:97.14%\n",
      "macro-recall:96.98%\n",
      "macro-averaged f1-score:97.05%\n"
     ]
    }
   ],
   "source": [
    "#Testing with the development set\n",
    "nb_classifier_test(train_count_vectorizer, train_category, dev_count_vectorizer, dev_category)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediction of test sets using models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:\n",
      "Accuracy: 98.43%\n",
      "macro-averaged precision:98.36%\n",
      "macro-recall:98.51%\n",
      "macro-averaged f1-score:98.42%\n"
     ]
    }
   ],
   "source": [
    "#Test Set Validation\n",
    "nb_classifier_test(train_count_vectorizer, train_category, test_count_vectorizer, test_category)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
